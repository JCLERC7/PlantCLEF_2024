{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import timm.utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Function to set the seed for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
    "    random.seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "    # The below two lines are for deterministic algorithm behavior in CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data/training/light_dataset\"\n",
    "nbr_classes = len(os.listdir(data_dir))\n",
    "nbr_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_mapping(class_list_file):\n",
    "    with open(class_list_file) as f:\n",
    "        class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "    return class_index_to_class_name\n",
    "\n",
    "\n",
    "def load_species_mapping(species_map_file):\n",
    "    df = pd.read_csv(species_map_file, sep=';', quoting=1, dtype={'species_id': str})\n",
    "    df = df.set_index('species_id')\n",
    "    return  df['species'].to_dict()\n",
    "\n",
    "cid_to_spid = load_class_mapping(\"models/pretrained_models/class_mapping.txt\")\n",
    "spid_to_sp = load_species_mapping(\"models/pretrained_models/species_id_to_name.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    # Resize the images to 518x518\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # p = probability of flip, 0.5 = 50% chance\n",
    "    # Add your custom augmentation here\n",
    "    transforms.RandomApply([transforms.TrivialAugmentWide(num_magnitude_bins=31)], p=0.5),\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def one_hot(label_idx, nbr_classes):\n",
    "    one_hot_tensor = torch.zeros(nbr_classes)\n",
    "    one_hot_tensor.scatter_(0, torch.tensor(label_idx), value=1)\n",
    "    return one_hot_tensor\n",
    "\n",
    "label_transformation = transforms.Compose([transforms.Lambda(lambda y: one_hot(y, nbr_classes))])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_dir,\n",
    "                            transform=data_transform,\n",
    "                            target_transform= label_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor:\n",
      "tensor([[[-0.6452, -0.6623, -0.7822,  ..., -0.4054, -0.6109, -1.0562],\n",
      "         [-1.1075, -0.8849, -0.7822,  ..., -0.6109, -0.8335, -1.0390],\n",
      "         [-1.7754, -1.6555, -1.4158,  ..., -0.9020, -1.0733, -0.7479],\n",
      "         ...,\n",
      "         [-1.6042, -1.6042, -1.3644,  ..., -0.7137, -0.6965, -0.6623],\n",
      "         [-0.7822, -0.7479, -0.7822,  ..., -0.7650, -0.6965, -0.6794],\n",
      "         [-0.9192, -1.2274, -1.6898,  ..., -0.5596, -0.6452, -0.6281]],\n",
      "\n",
      "        [[ 0.7479,  0.7479,  0.6254,  ...,  0.5203,  0.3978,  0.1877],\n",
      "         [-0.0924,  0.2227,  0.4328,  ...,  0.1176, -0.0224,  0.0126],\n",
      "         [-1.1253, -0.8627, -0.4251,  ..., -0.3025, -0.3901,  0.2052],\n",
      "         ...,\n",
      "         [-0.9153, -0.8627, -0.6001,  ...,  0.6954,  0.7129,  0.7479],\n",
      "         [ 0.4328,  0.4678,  0.4153,  ...,  0.6429,  0.6954,  0.7129],\n",
      "         [ 0.0651, -0.3200, -0.8978,  ...,  0.8354,  0.7304,  0.7479]],\n",
      "\n",
      "        [[-0.4624, -0.4798, -0.6367,  ..., -0.3055, -0.5495, -1.0376],\n",
      "         [-0.9330, -0.7238, -0.6541,  ..., -0.6715, -0.8807, -1.1073],\n",
      "         [-1.5953, -1.5256, -1.3513,  ..., -0.9330, -1.0898, -0.7587],\n",
      "         ...,\n",
      "         [-1.4733, -1.5256, -1.4384,  ..., -0.0092,  0.0256,  0.0605],\n",
      "         [-1.1770, -1.1073, -1.1247,  ...,  0.0256,  0.0953,  0.1128],\n",
      "         [-1.2293, -1.4036, -1.5604,  ...,  0.2871,  0.2173,  0.2522]]])\n",
      "Image shape: torch.Size([3, 224, 224])\n",
      "Image datatype: torch.float32\n",
      "Image label: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Label datatype: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "img, label = dataset[0][0], dataset[0][1]\n",
    "print(f\"Image tensor:\\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer( experiment_name: str, model_name: str, extra: str=None):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    if extra:\n",
    "        # Create log directory path\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "    \n",
    "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: runs\\2024-06-13\\Run_5\\vit_small_patch14_reg4_dinov2_img224\\lr-8.0e-05_epoch-100_batch-64_light_dataset...\n"
     ]
    }
   ],
   "source": [
    "writer = create_writer(\"Run_5\", \"vit_small_patch14_reg4_dinov2_img224\", \"lr-8.0e-05_epoch-100_batch-64_light_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreatDataloader(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch:int\n",
    "    ) -> None:\n",
    "        TRAIN_PERCENT = 0.8\n",
    "        self.dataset = dataset\n",
    "        self.batch = batch\n",
    "        self.dataset_size = len(dataset)\n",
    "        self.train_size = int(TRAIN_PERCENT * self.dataset_size)\n",
    "        # self.test_size = int(0.01 * self.dataset_size)\n",
    "        self.test_size = int((self.dataset_size - self.train_size)/2)\n",
    "        self.validation_size = self.dataset_size - self.train_size - self.test_size\n",
    "        self.train_data, self.test_data, self.validation_data = torch.utils.data.random_split(self.dataset,\n",
    "                                                                                              [self.train_size, self.test_size, self.validation_size])\n",
    "        \n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(self.train_data,\n",
    "                          batch_size=self.batch,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True)\n",
    "    \n",
    "    def get_test_dataloader(self):\n",
    "        return DataLoader(self.test_data,\n",
    "                          batch_size=self.batch,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True)\n",
    "        \n",
    "    def get_validation_dataloader(self):\n",
    "        return DataLoader(self.validation_data,\n",
    "                          batch_size=self.batch,\n",
    "                          shuffle=False,\n",
    "                          pin_memory=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "dataloader = CreatDataloader(dataset, batch=64)\n",
    "training_dataloader = dataloader.get_train_dataloader()\n",
    "test_dataloader = dataloader.get_test_dataloader()\n",
    "validation_dataloader = dataloader.get_validation_dataloader()\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "model = timm.create_model('vit_small_patch14_reg4_dinov2.lvd142m',\n",
    "                          pretrained=True,\n",
    "                          num_classes=0,\n",
    "                          img_size=224)\n",
    "\n",
    "model.head = nn.Sequential(nn.Linear(model.num_features, nbr_classes), nn.Sigmoid())\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=8.0e-05)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_data: DataLoader,\n",
    "        test_data: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_fn: torch.nn.Module,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        writer: SummaryWriter\n",
    "    ) -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.epochs_run = 0\n",
    "        self.scheduler = scheduler\n",
    "        self.writer = writer\n",
    "        self.step = 0\n",
    "\n",
    "    def _run_batch(self, source, targets, idx):\n",
    "        NUM_ACCUMULATION_STEPS = 10\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(source)\n",
    "        loss = self.loss_fn(output, targets)\n",
    "        # loss = loss / NUM_ACCUMULATION_STEPS\n",
    "        train_pred_labels = torch.round(output)\n",
    "        loss.backward()\n",
    "        # if ((idx+1) % NUM_ACCUMULATION_STEPS == 0) or (idx+1 == len(self.train_data)):\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.writer:\n",
    "            self.writer.add_scalar(\"Batch/Loss\", loss.item(), self.step)\n",
    "        self.step = self.step+1\n",
    "        return loss.item(), ((train_pred_labels == targets).sum().item()/torch.numel(train_pred_labels))\n",
    "    # print(torch.all(y==sample[1].to(device), dim=1).sum().item()/len(y))\n",
    "        \n",
    "        \n",
    "    def _test_batch(self, source, targets):\n",
    "        test_output = self.model(source)\n",
    "        loss = self.loss_fn(test_output, targets)\n",
    "        test_pred_labels = torch.round(test_output)\n",
    "        return loss.item(), ((test_pred_labels == targets).sum().item()/torch.numel(test_pred_labels))\n",
    "        \n",
    "\n",
    "    def _run_epoch(self, epoch):\n",
    "        test_loss, test_acc = 0, 0\n",
    "        train_loss, train_acc = 0, 0\n",
    "        b_sz = len(next(iter(self.train_data))[0])\n",
    "        print(f\"[GPU{device}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)} | Time: {datetime.now()}\")\n",
    "        for idx, sample in enumerate(self.train_data):\n",
    "            source = sample[0].to(device)\n",
    "            targets = sample[1].to(device)\n",
    "            loss, accu = self._run_batch(source, targets, idx)\n",
    "            train_loss += loss\n",
    "            train_acc += accu\n",
    "        self.scheduler.step()\n",
    "        train_loss = train_loss / len(self.train_data)\n",
    "        train_acc = train_acc / len(self.train_data)\n",
    "            \n",
    "        if epoch % 3 == 0:\n",
    "            self.model.eval()\n",
    "            with torch.inference_mode():\n",
    "                for source, targets in self.test_data:\n",
    "                    source = source.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    loss, accu = self._test_batch(source, targets)\n",
    "                    test_loss += loss\n",
    "                    test_acc += accu\n",
    "            test_loss = test_loss / len(self.test_data)\n",
    "            test_acc = test_acc / len(self.test_data)\n",
    "            if self.writer:\n",
    "                self.writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "                self.writer.add_scalar(\"Accuracy/test\", test_acc, epoch)\n",
    "            \n",
    "            \n",
    "        if self.writer:\n",
    "            self.writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "            self.writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def train(self, max_epochs: int):\n",
    "        for epoch in range(self.epochs_run, max_epochs):\n",
    "            self._run_epoch(epoch)\n",
    "        # Close the writer\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "Trainer = Trainer(model=model,\n",
    "                  train_data=training_dataloader,\n",
    "                  test_data=test_dataloader,\n",
    "                  optimizer=optimizer,\n",
    "                  loss_fn=loss_fn,\n",
    "                  scheduler=scheduler,\n",
    "                  writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPUcuda] Epoch 0 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-13 18:00:46.961622\n",
      "[GPUcuda] Epoch 1 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-13 19:57:33.452960\n",
      "[GPUcuda] Epoch 2 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-13 21:06:46.751882\n",
      "[GPUcuda] Epoch 3 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-13 21:50:59.899522\n",
      "[GPUcuda] Epoch 4 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-13 23:04:38.323980\n",
      "[GPUcuda] Epoch 5 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 00:07:03.371167\n",
      "[GPUcuda] Epoch 6 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 00:49:17.196851\n",
      "[GPUcuda] Epoch 7 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 01:39:08.669699\n",
      "[GPUcuda] Epoch 8 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 02:37:25.936669\n",
      "[GPUcuda] Epoch 9 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 03:19:18.199660\n",
      "[GPUcuda] Epoch 10 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 04:16:38.161554\n",
      "[GPUcuda] Epoch 11 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 05:18:01.118576\n",
      "[GPUcuda] Epoch 12 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 06:17:09.122546\n",
      "[GPUcuda] Epoch 13 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 07:16:53.956095\n",
      "[GPUcuda] Epoch 14 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 08:16:55.616071\n",
      "[GPUcuda] Epoch 15 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 09:18:29.897202\n",
      "[GPUcuda] Epoch 16 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 10:21:44.735445\n",
      "[GPUcuda] Epoch 17 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 11:32:50.104665\n",
      "[GPUcuda] Epoch 18 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 12:17:28.679955\n",
      "[GPUcuda] Epoch 19 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 13:05:24.862462\n",
      "[GPUcuda] Epoch 20 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 13:53:26.197772\n",
      "[GPUcuda] Epoch 21 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 14:31:25.011355\n",
      "[GPUcuda] Epoch 22 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 15:18:27.749106\n",
      "[GPUcuda] Epoch 23 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 16:08:52.326382\n",
      "[GPUcuda] Epoch 24 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 16:46:24.931915\n",
      "[GPUcuda] Epoch 25 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 17:32:04.678658\n",
      "[GPUcuda] Epoch 26 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 18:22:26.340271\n",
      "[GPUcuda] Epoch 27 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 18:59:59.683987\n",
      "[GPUcuda] Epoch 28 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 19:45:20.766372\n",
      "[GPUcuda] Epoch 29 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 20:36:11.782394\n",
      "[GPUcuda] Epoch 30 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 21:13:45.353022\n",
      "[GPUcuda] Epoch 31 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 22:00:01.085418\n",
      "[GPUcuda] Epoch 32 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 23:06:21.867550\n",
      "[GPUcuda] Epoch 33 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-14 23:49:12.859977\n",
      "[GPUcuda] Epoch 34 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 00:36:57.525094\n",
      "[GPUcuda] Epoch 35 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 01:32:21.627403\n",
      "[GPUcuda] Epoch 36 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 02:10:04.270739\n",
      "[GPUcuda] Epoch 37 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 02:53:32.781266\n",
      "[GPUcuda] Epoch 38 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 03:31:00.946193\n",
      "[GPUcuda] Epoch 39 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 04:08:27.411453\n",
      "[GPUcuda] Epoch 40 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 04:48:51.640355\n",
      "[GPUcuda] Epoch 41 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 05:26:22.871600\n",
      "[GPUcuda] Epoch 42 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 06:17:24.471709\n",
      "[GPUcuda] Epoch 43 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 07:03:56.176991\n",
      "[GPUcuda] Epoch 44 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 07:41:21.273342\n",
      "[GPUcuda] Epoch 45 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 08:18:46.548865\n",
      "[GPUcuda] Epoch 46 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 09:00:52.430001\n",
      "[GPUcuda] Epoch 47 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 10:01:22.429789\n",
      "[GPUcuda] Epoch 48 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 11:17:05.823460\n",
      "[GPUcuda] Epoch 49 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 12:27:22.544405\n",
      "[GPUcuda] Epoch 50 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 13:31:25.132874\n",
      "[GPUcuda] Epoch 51 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 14:25:25.545489\n",
      "[GPUcuda] Epoch 52 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 15:29:03.802743\n",
      "[GPUcuda] Epoch 53 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 16:31:47.752410\n",
      "[GPUcuda] Epoch 54 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 17:20:57.475069\n",
      "[GPUcuda] Epoch 55 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 18:12:21.244628\n",
      "[GPUcuda] Epoch 56 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 19:11:57.356312\n",
      "[GPUcuda] Epoch 57 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 20:00:05.772566\n",
      "[GPUcuda] Epoch 58 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 20:54:47.904691\n",
      "[GPUcuda] Epoch 59 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 21:59:51.361036\n",
      "[GPUcuda] Epoch 60 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-15 22:58:12.577324\n",
      "[GPUcuda] Epoch 61 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 00:06:55.249701\n",
      "[GPUcuda] Epoch 62 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 01:10:05.424383\n",
      "[GPUcuda] Epoch 63 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 01:57:49.668260\n",
      "[GPUcuda] Epoch 64 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 02:53:28.768994\n",
      "[GPUcuda] Epoch 65 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 03:54:51.399279\n",
      "[GPUcuda] Epoch 66 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 04:43:17.435560\n",
      "[GPUcuda] Epoch 67 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 05:44:08.743412\n",
      "[GPUcuda] Epoch 68 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 06:52:02.328449\n",
      "[GPUcuda] Epoch 69 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 07:34:23.853091\n",
      "[GPUcuda] Epoch 70 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 08:24:03.703705\n",
      "[GPUcuda] Epoch 71 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 09:22:26.050296\n",
      "[GPUcuda] Epoch 72 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 10:11:33.800056\n",
      "[GPUcuda] Epoch 73 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 11:05:30.050618\n",
      "[GPUcuda] Epoch 74 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 12:11:23.502100\n",
      "[GPUcuda] Epoch 75 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 13:08:12.097935\n",
      "[GPUcuda] Epoch 76 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 13:59:02.738475\n",
      "[GPUcuda] Epoch 77 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 14:51:54.671708\n",
      "[GPUcuda] Epoch 78 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 15:53:34.085142\n",
      "[GPUcuda] Epoch 79 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 16:52:29.135273\n",
      "[GPUcuda] Epoch 80 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 17:46:19.457706\n",
      "[GPUcuda] Epoch 81 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 18:23:12.004038\n",
      "[GPUcuda] Epoch 82 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 19:08:35.915607\n",
      "[GPUcuda] Epoch 83 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 20:02:59.704597\n",
      "[GPUcuda] Epoch 84 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 20:59:44.102163\n",
      "[GPUcuda] Epoch 85 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 21:57:18.767948\n",
      "[GPUcuda] Epoch 86 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-16 22:54:06.639137\n",
      "[GPUcuda] Epoch 87 | Batchsize: 64 | Steps: 2505 | Time: 2024-06-17 00:24:37.539343\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "Trainer.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model\u001b[39m(model: \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m      2\u001b[0m                target_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m      3\u001b[0m                model_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Saves a PyTorch model to a target directory.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m               model_name=\"05_going_modular_tingvgg_model.pth\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Create target directory\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "    \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "    Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "\n",
    "    Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "    \"\"\"\n",
    "    # Create target directory\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    # Save the model state_dict()\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model=model, target_dir=\"models/final_models\", model_name=\"Dino_Run1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# output = model(ten_img)\n",
    "# print(output[0].argmax())\n",
    "# arg = int(output[0].argmax().cpu().detach().numpy())\n",
    "# print(arg)\n",
    "# print(spid_to_sp[cid_to_spid[arg]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top5_probabilities, top5_class_indices = torch.topk(output[0] * 100, k=5)\n",
    "# top5_probabilities = top5_probabilities.cpu().detach().numpy()\n",
    "# top5_class_indices = top5_class_indices.cpu().detach().numpy()\n",
    "\n",
    "# print(top5_probabilities)\n",
    "\n",
    "# for proba, cid in zip(top5_probabilities, top5_class_indices):\n",
    "#     species_id = cid_to_spid[cid]\n",
    "#     species = spid_to_sp[species_id]\n",
    "#     print(species_id, species, proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
